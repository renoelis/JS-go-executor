package service

import (
	"context"
	"encoding/json"
	"fmt"
	"sync"
	"time"

	"flow-codeblock-go/utils"

	"github.com/jmoiron/sqlx"
	"github.com/redis/go-redis/v9"
	"go.uber.org/zap"
)

// ==================== çƒ­æ•°æ®å±‚ï¼ˆå†…å­˜ï¼‰ ====================

// HotDataTier çƒ­æ•°æ®å±‚ - å†…å­˜å­˜å‚¨ï¼ˆæœ€è¿‘5åˆ†é’Ÿæ´»è·ƒTokenï¼‰
type HotDataTier struct {
	data        map[string][]int64   // tokenKey -> requestTimestamps[]
	accessOrder map[string]time.Time // tokenKey -> lastAccessTime (LRU)
	maxTokens   int
	mutex       sync.RWMutex
}

// NewHotDataTier åˆ›å»ºçƒ­æ•°æ®å±‚
func NewHotDataTier(maxTokens int) *HotDataTier {
	return &HotDataTier{
		data:        make(map[string][]int64),
		accessOrder: make(map[string]time.Time),
		maxTokens:   maxTokens,
	}
}

// Get èŽ·å–è¯·æ±‚åŽ†å²
func (h *HotDataTier) Get(tokenKey string) ([]int64, bool) {
	h.mutex.RLock()
	data, exists := h.data[tokenKey]
	if !exists {
		h.mutex.RUnlock()
		return nil, false
	}
	h.mutex.RUnlock()

	// æ›´æ–°è®¿é—®æ—¶é—´ï¼ˆéœ€è¦å†™é”ï¼‰
	h.mutex.Lock()
	h.accessOrder[tokenKey] = time.Now()
	h.mutex.Unlock()

	return data, true
}

// Set è®¾ç½®è¯·æ±‚åŽ†å²
func (h *HotDataTier) Set(tokenKey string, requests []int64) {
	h.mutex.Lock()
	defer h.mutex.Unlock()

	// LRUæ·˜æ±°ç­–ç•¥
	if len(h.data) >= h.maxTokens && h.data[tokenKey] == nil {
		h.evictLRU()
	}

	h.data[tokenKey] = requests
	h.accessOrder[tokenKey] = time.Now()
}

// Delete åˆ é™¤Tokenæ•°æ®
func (h *HotDataTier) Delete(tokenKey string) {
	h.mutex.Lock()
	defer h.mutex.Unlock()

	delete(h.data, tokenKey)
	delete(h.accessOrder, tokenKey)
}

// Cleanup æ¸…ç†è¿‡æœŸæ•°æ®
func (h *HotDataTier) Cleanup(cleanupAge time.Duration) {
	h.mutex.Lock()
	defer h.mutex.Unlock()

	now := time.Now()
	cleanupThreshold := now.Add(-cleanupAge)

	for tokenKey, lastAccess := range h.accessOrder {
		if lastAccess.Before(cleanupThreshold) {
			delete(h.data, tokenKey)
			delete(h.accessOrder, tokenKey)
		}
	}
}

// evictLRU LRUæ·˜æ±°
func (h *HotDataTier) evictLRU() {
	var oldestToken string
	var oldestTime time.Time

	for tokenKey, lastAccess := range h.accessOrder {
		if oldestToken == "" || lastAccess.Before(oldestTime) {
			oldestToken = tokenKey
			oldestTime = lastAccess
		}
	}

	if oldestToken != "" {
		delete(h.data, oldestToken)
		delete(h.accessOrder, oldestToken)
		utils.Debug("çƒ­æ•°æ®å±‚LRUæ·˜æ±°", zap.String("token_key", oldestToken[:min(15, len(oldestToken))]+"***"))
	}
}

// GetStats èŽ·å–ç»Ÿè®¡ä¿¡æ¯
func (h *HotDataTier) GetStats() map[string]interface{} {
	h.mutex.RLock()
	defer h.mutex.RUnlock()

	return map[string]interface{}{
		"size":                len(h.data),
		"max_size":            h.maxTokens,
		"utilization_percent": int(float64(len(h.data)) / float64(h.maxTokens) * 100),
	}
}

// ==================== æ¸©æ•°æ®å±‚ï¼ˆRedisï¼‰ ====================

// WarmDataTier æ¸©æ•°æ®å±‚ - Rediså­˜å‚¨ï¼ˆæœ€è¿‘1å°æ—¶æ´»è·ƒTokenï¼‰
type WarmDataTier struct {
	redis      *redis.Client
	enabled    bool
	keyPrefix  string
	ttl        time.Duration
	errorCount int
	maxErrors  int
	mutex      sync.RWMutex
}

// NewWarmDataTier åˆ›å»ºæ¸©æ•°æ®å±‚
func NewWarmDataTier(redisClient *redis.Client, ttl time.Duration) *WarmDataTier {
	return &WarmDataTier{
		redis:      redisClient,
		enabled:    redisClient != nil,
		keyPrefix:  "rate_limit:",
		ttl:        ttl,
		maxErrors:  5,
		errorCount: 0,
	}
}

// Get èŽ·å–è¯·æ±‚åŽ†å²
func (w *WarmDataTier) Get(ctx context.Context, tokenKey string) ([]int64, bool) {
	if !w.isAvailable() {
		return nil, false
	}

	key := w.keyPrefix + tokenKey
	data, err := w.redis.Get(ctx, key).Result()
	if err != nil {
		if err != redis.Nil {
			w.handleError(err, "è¯»å–")
		}
		return nil, false
	}

	var requests []int64
	if err := json.Unmarshal([]byte(data), &requests); err != nil {
		utils.Error("æ¸©æ•°æ®å±‚æ•°æ®ååºåˆ—åŒ–å¤±è´¥", zap.Error(err))
		return nil, false
	}

	// æˆåŠŸæ“ä½œï¼Œé‡ç½®é”™è¯¯è®¡æ•°
	w.resetErrorCount()
	return requests, true
}

// Set è®¾ç½®è¯·æ±‚åŽ†å²
func (w *WarmDataTier) Set(ctx context.Context, tokenKey string, requests []int64) error {
	if !w.isAvailable() {
		return nil
	}

	key := w.keyPrefix + tokenKey
	jsonData, err := json.Marshal(requests)
	if err != nil {
		return err
	}

	if err := w.redis.Set(ctx, key, jsonData, w.ttl).Err(); err != nil {
		w.handleError(err, "å†™å…¥")
		return err
	}

	// æˆåŠŸæ“ä½œï¼Œé‡ç½®é”™è¯¯è®¡æ•°
	w.resetErrorCount()
	return nil
}

// Delete åˆ é™¤Tokenæ•°æ®
func (w *WarmDataTier) Delete(ctx context.Context, tokenKey string) error {
	if !w.isAvailable() {
		return nil
	}

	key := w.keyPrefix + tokenKey
	return w.redis.Del(ctx, key).Err()
}

// isAvailable æ£€æŸ¥Redisæ˜¯å¦å¯ç”¨
func (w *WarmDataTier) isAvailable() bool {
	w.mutex.RLock()
	defer w.mutex.RUnlock()
	return w.enabled && w.redis != nil
}

// handleError å¤„ç†Redisé”™è¯¯
func (w *WarmDataTier) handleError(err error, operation string) {
	w.mutex.Lock()
	defer w.mutex.Unlock()

	utils.Error("æ¸©æ•°æ®å±‚"+operation+"å¤±è´¥", zap.Error(err))
	w.errorCount++

	// è¿žç»­é”™è¯¯è¾¾åˆ°é˜ˆå€¼ï¼Œä¸´æ—¶ç¦ç”¨Redis
	if w.errorCount >= w.maxErrors {
		w.enabled = false
		utils.Warn("æ¸©æ•°æ®å±‚è¿žç»­é”™è¯¯ï¼Œå·²ä¸´æ—¶ç¦ç”¨", zap.Int("error_count", w.errorCount))
	}
}

// resetErrorCount é‡ç½®é”™è¯¯è®¡æ•°
func (w *WarmDataTier) resetErrorCount() {
	w.mutex.Lock()
	defer w.mutex.Unlock()

	if w.errorCount > 0 {
		w.errorCount = 0
	}
}

// GetStats èŽ·å–ç»Ÿè®¡ä¿¡æ¯
func (w *WarmDataTier) GetStats(ctx context.Context) map[string]interface{} {
	if !w.isAvailable() {
		return map[string]interface{}{
			"enabled": false,
		}
	}

	keys, err := w.redis.Keys(ctx, w.keyPrefix+"*").Result()
	if err != nil {
		return map[string]interface{}{
			"enabled": true,
			"error":   err.Error(),
		}
	}

	return map[string]interface{}{
		"enabled": true,
		"size":    len(keys),
		"ttl":     int(w.ttl.Seconds()),
	}
}

// ==================== å†·æ•°æ®å±‚ï¼ˆMySQLï¼‰ ====================

// ColdDataTier å†·æ•°æ®å±‚ - æ•°æ®åº“å­˜å‚¨ï¼ˆåŽ†å²æ•°æ®ï¼‰
type ColdDataTier struct {
	db          *sqlx.DB
	tableName   string
	batchBuffer map[string]*batchItem
	batchSize   int
	mutex       sync.RWMutex

	// ðŸ”¥ æ–°å¢žï¼šä¼˜é›…å…³é—­æ”¯æŒ
	shutdown chan struct{}
	wg       sync.WaitGroup
}

type batchItem struct {
	TokenKey       string
	RequestCount   int
	TimeWindow     int
	RecordDate     string
	RecordHour     int
	FirstRequestAt int64
	LastRequestAt  int64
}

// NewColdDataTier åˆ›å»ºå†·æ•°æ®å±‚
func NewColdDataTier(db *sqlx.DB, batchSize int) *ColdDataTier {
	tier := &ColdDataTier{
		db:          db,
		tableName:   "token_rate_limit_history",
		batchBuffer: make(map[string]*batchItem),
		batchSize:   batchSize,
		shutdown:    make(chan struct{}), // ðŸ”¥ åˆå§‹åŒ–å…³é—­ä¿¡å·
	}

	// å¯åŠ¨æ‰¹é‡å†™å…¥å®šæ—¶å™¨
	tier.wg.Add(1) // ðŸ”¥ æ³¨å†Œ goroutine
	go tier.startBatchTimer()

	return tier
}

// Set è®¾ç½®è¯·æ±‚åŽ†å²ï¼ˆæ‰¹é‡å†™å…¥ï¼‰
func (c *ColdDataTier) Set(ctx context.Context, tokenKey string, requests []int64) error {
	if len(requests) == 0 {
		return nil
	}

	// æŒ‰å°æ—¶èšåˆæ•°æ®
	hourlyData := c.aggregateByHour(requests)

	c.mutex.Lock()
	defer c.mutex.Unlock()

	for hourKey, data := range hourlyData {
		bufferKey := fmt.Sprintf("%s_%s", tokenKey, hourKey)

		if existing, exists := c.batchBuffer[bufferKey]; exists {
			// åˆå¹¶çŽ°æœ‰æ•°æ®
			existing.RequestCount += data.RequestCount
			if data.LastRequestAt > existing.LastRequestAt {
				existing.LastRequestAt = data.LastRequestAt
			}
			if data.FirstRequestAt < existing.FirstRequestAt {
				existing.FirstRequestAt = data.FirstRequestAt
			}
		} else {
			data.TokenKey = tokenKey
			c.batchBuffer[bufferKey] = data
		}
	}

	// å¦‚æžœç¼“å†²åŒºæ»¡äº†ï¼Œè§¦å‘æ‰¹é‡å†™å…¥
	if len(c.batchBuffer) >= c.batchSize {
		// ðŸ”¥ ä½¿ç”¨ WaitGroup è¿½è¸ªä¸´æ—¶ goroutineï¼Œç¡®ä¿ä¼˜é›…å…³é—­æ—¶ç­‰å¾…å®Œæˆ
		c.wg.Add(1)
		go func() {
			defer c.wg.Done()
			c.flushBatch()
		}()
	}

	return nil
}

// aggregateByHour æŒ‰å°æ—¶èšåˆæ•°æ®
func (c *ColdDataTier) aggregateByHour(requests []int64) map[string]*batchItem {
	hourlyData := make(map[string]*batchItem)

	for _, timestamp := range requests {
		t := time.UnixMilli(timestamp)
		dateStr := t.Format("2006-01-02")
		hour := t.Hour()
		hourKey := fmt.Sprintf("%s_%d", dateStr, hour)

		if item, exists := hourlyData[hourKey]; exists {
			item.RequestCount++
			if timestamp > item.LastRequestAt {
				item.LastRequestAt = timestamp
			}
			if timestamp < item.FirstRequestAt {
				item.FirstRequestAt = timestamp
			}
		} else {
			hourlyData[hourKey] = &batchItem{
				RecordDate:     dateStr,
				RecordHour:     hour,
				RequestCount:   1,
				TimeWindow:     3600,
				FirstRequestAt: timestamp,
				LastRequestAt:  timestamp,
			}
		}
	}

	return hourlyData
}

// flushBatch æ‰¹é‡å†™å…¥æ•°æ®åº“
func (c *ColdDataTier) flushBatch() {
	c.mutex.Lock()
	if len(c.batchBuffer) == 0 {
		c.mutex.Unlock()
		return
	}

	// å¤åˆ¶ç¼“å†²åŒºæ•°æ®
	items := make([]*batchItem, 0, len(c.batchBuffer))
	for _, item := range c.batchBuffer {
		items = append(items, item)
	}
	c.batchBuffer = make(map[string]*batchItem)
	c.mutex.Unlock()

	// æ‰¹é‡æ’å…¥
	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	for _, item := range items {
		query := `
			INSERT INTO ` + c.tableName + ` (
				token_key, request_count, time_window, record_date, record_hour,
				first_request_at, last_request_at
			) VALUES (?, ?, ?, ?, ?, ?, ?)
			ON DUPLICATE KEY UPDATE
				request_count = request_count + VALUES(request_count),
				last_request_at = GREATEST(last_request_at, VALUES(last_request_at)),
				first_request_at = LEAST(first_request_at, VALUES(first_request_at))
		`

		_, err := c.db.ExecContext(ctx, query,
			item.TokenKey,
			item.RequestCount,
			item.TimeWindow,
			item.RecordDate,
			item.RecordHour,
			time.UnixMilli(item.FirstRequestAt),
			time.UnixMilli(item.LastRequestAt),
		)
		if err != nil {
			utils.Error("å†·æ•°æ®å±‚æ‰¹é‡å†™å…¥å¤±è´¥", zap.Error(err))
		}
	}

	utils.Debug("å†·æ•°æ®å±‚æ‰¹é‡å†™å…¥å®Œæˆ", zap.Int("count", len(items)))
}

// startBatchTimer å¯åŠ¨æ‰¹é‡å†™å…¥å®šæ—¶å™¨
func (c *ColdDataTier) startBatchTimer() {
	defer c.wg.Done() // ðŸ”¥ goroutine é€€å‡ºæ—¶é€šçŸ¥

	ticker := time.NewTicker(60 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			c.flushBatch()
		case <-c.shutdown: // ðŸ”¥ ç›‘å¬å…³é—­ä¿¡å·
			// ðŸ”¥ å…³é—­å‰æœ€åŽä¸€æ¬¡åˆ·æ–°ï¼Œé¿å…æ•°æ®ä¸¢å¤±
			utils.Info("ColdDataTier æ­£åœ¨å…³é—­ï¼Œæ‰§è¡Œæœ€åŽä¸€æ¬¡æ‰¹é‡åˆ·æ–°")
			c.flushBatch()
			utils.Info("ColdDataTier æ‰¹é‡å†™å…¥å®šæ—¶å™¨å·²åœæ­¢")
			return
		}
	}
}

// GetStats èŽ·å–ç»Ÿè®¡ä¿¡æ¯
func (c *ColdDataTier) GetStats() map[string]interface{} {
	c.mutex.RLock()
	defer c.mutex.RUnlock()

	return map[string]interface{}{
		"enabled":           true,
		"batch_buffer_size": len(c.batchBuffer),
		"batch_size":        c.batchSize,
		"table_name":        c.tableName,
	}
}

// Close å…³é—­å†·æ•°æ®å±‚ï¼Œç­‰å¾…æ‰€æœ‰æ‰¹é‡å†™å…¥å®Œæˆ
func (c *ColdDataTier) Close() error {
	utils.Info("å¼€å§‹å…³é—­ ColdDataTier")

	// å‘é€å…³é—­ä¿¡å·
	close(c.shutdown)

	// ç­‰å¾… goroutine é€€å‡º
	c.wg.Wait()

	utils.Info("ColdDataTier å·²å…³é—­")
	return nil
}
