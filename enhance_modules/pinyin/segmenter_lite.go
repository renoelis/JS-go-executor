package pinyin

import (
	"flow-codeblock-go/enhance_modules/pinyin/core"
	"flow-codeblock-go/enhance_modules/pinyin/dict"
	"flow-codeblock-go/enhance_modules/pinyin/optimizer"
	"flow-codeblock-go/enhance_modules/pinyin/tokenizer"
)

// LightweightSegmenter è½»é‡çº§åˆ†è¯å™¨ï¼ˆä¸ä¾èµ– GSEï¼‰
// åŸºäºå†…ç½®è¯ç»„å­—å…¸è¿›è¡Œåˆ†è¯ï¼Œå†…å­˜å ç”¨æä½
type LightweightSegmenter struct {
	tokenizerPipeline *tokenizer.Pipeline // ğŸ”¥ Tokenizer ç®¡é“
	optimizerPipeline *optimizer.Pipeline // ğŸ”¥ Optimizer ç®¡é“
}

// NewLightweightSegmenter åˆ›å»ºè½»é‡çº§åˆ†è¯å™¨
func NewLightweightSegmenter() *LightweightSegmenter {
	// ğŸ”¥ åˆå§‹åŒ– Tokenizer ç®¡é“ï¼ˆ100% å…¼å®¹ npm pinyin v4ï¼‰
	// 
	// â­ é‡è¦æ›´æ–°ï¼šç»è¿‡å¯¹ JS åŸç‰ˆçš„æ·±åº¦åˆ†æï¼Œnpm pinyin v4 ä½¿ç”¨äº†å®Œæ•´çš„ Tokenizer ç®¡é“ï¼
	// 
	// JS åŸç‰ˆä½¿ç”¨çš„æ¨¡å—é¡ºåºï¼ˆmodules æ•°ç»„ï¼‰ï¼š
	//   var modules = [
	//     URLTokenizer,          // URLè¯†åˆ«
	//     WildcardTokenizer,     // é€šé…ç¬¦ï¼Œå¿…é¡»åœ¨æ ‡ç‚¹ç¬¦å·è¯†åˆ«ä¹‹å‰
	//     PunctuationTokenizer,  // æ ‡ç‚¹ç¬¦å·è¯†åˆ«
	//     ForeignTokenizer,      // å¤–æ–‡å­—ç¬¦ã€æ•°å­—è¯†åˆ«ï¼Œå¿…é¡»åœ¨æ ‡ç‚¹ç¬¦å·è¯†åˆ«ä¹‹å
	//     DictTokenizer,         // è¯å…¸è¯†åˆ«
	//     ChsNameTokenizer,      // äººåè¯†åˆ«ï¼Œå»ºè®®åœ¨è¯å…¸è¯†åˆ«ä¹‹å
	//     // ä¼˜åŒ–æ¨¡å—
	//     EmailOptimizer,        // é‚®ç®±åœ°å€è¯†åˆ«
	//     ChsNameOptimizer,      // äººåè¯†åˆ«ä¼˜åŒ–
	//     DictOptimizer,         // è¯å…¸è¯†åˆ«ä¼˜åŒ–
	//     DatetimeOptimizer,     // æ—¥æœŸæ—¶é—´è¯†åˆ«ä¼˜åŒ–
	//     AdjectiveOptimizer     // å½¢å®¹è¯ä¼˜åŒ–
	//   ];
	//
	// å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¯ç”¨æ‰€æœ‰ Tokenizer ä»¥ä¿æŒ 100% å…¼å®¹ï¼š
	//
	tokenizerPipeline := tokenizer.NewPipeline(
		tokenizer.NewURLTokenizer(),         // âœ… URLè¯†åˆ«
		tokenizer.NewWildcardTokenizer(),    // âœ… é€šé…ç¬¦è¯†åˆ«ï¼ˆå¿…é¡»åœ¨æ ‡ç‚¹ä¹‹å‰ï¼‰
		tokenizer.NewPunctuationTokenizer(), // âœ… æ ‡ç‚¹ç¬¦å·è¯†åˆ«
		tokenizer.NewForeignTokenizer(),     // âœ… å¤–æ–‡å­—ç¬¦ã€æ•°å­—è¯†åˆ«ï¼ˆå¿…é¡»åœ¨æ ‡ç‚¹ä¹‹åï¼‰
		tokenizer.NewChsNameTokenizer(),     // âœ… ä¸­æ–‡äººåè¯†åˆ«
	)

	// ğŸ”¥ åˆå§‹åŒ– Optimizer ç®¡é“ï¼ˆä¼˜åŒ–é˜¶æ®µï¼‰
	optimizerPipeline := optimizer.NewPipeline(
		optimizer.NewDatetimeOptimizer(),  // æ—¥æœŸæ—¶é—´è¯†åˆ«ä¼˜åŒ–
		optimizer.NewChsNameOptimizer(),   // äººåè¯†åˆ«ä¼˜åŒ–
		optimizer.NewDictOptimizer(),      // è¯å…¸ä¼˜åŒ–ï¼ˆMMSGç®—æ³•ï¼‰
		optimizer.NewAdjectiveOptimizer(), // å½¢å®¹è¯ä¼˜åŒ–
	)

	return &LightweightSegmenter{
		tokenizerPipeline: tokenizerPipeline,
		optimizerPipeline: optimizerPipeline,
	}
}

// Segment è½»é‡çº§åˆ†è¯ï¼ˆåŸºäºè¯ç»„å­—å…¸çš„å‰å‘æœ€å¤§åŒ¹é…ï¼‰
// å†…å­˜å ç”¨: ~0MBï¼ˆå¤ç”¨å·²æœ‰çš„ PhrasesDictï¼‰
func (ls *LightweightSegmenter) Segment(text string, mode string) []string {
	if text == "" {
		return []string{}
	}

	runes := []rune(text)
	result := make([]string, 0, len(runes))
	i := 0

	for i < len(runes) {
		// ğŸ¯ ç­–ç•¥1: å°è¯•æœ€é•¿è¯ç»„åŒ¹é…ï¼ˆå‰å‘æœ€å¤§åŒ¹é…ç®—æ³•ï¼‰
		maxLen := 8 // æœ€å¤§è¯ç»„é•¿åº¦ï¼ˆå¯è°ƒæ•´ï¼‰
		if maxLen > len(runes)-i {
			maxLen = len(runes) - i
		}

		matched := false
		// ä»æœ€é•¿åˆ°æœ€çŸ­å°è¯•åŒ¹é…
		for length := maxLen; length >= 2; length-- {
			if i+length > len(runes) {
				continue
			}

			phrase := string(runes[i : i+length])
			// â­ ä¿®å¤ï¼šåŒæ—¶æ£€æŸ¥ PhrasesDict å’Œ SpecialDict
			// JS åŸç‰ˆåœ¨ phrase_match.go ä¸­ä¹Ÿæ˜¯è¿™æ ·åšçš„
			_, foundInPhrases := core.GetPhraseFromDict(phrase)
			isSpecial := dict.IsSpecialWord(phrase)
			
			if foundInPhrases || isSpecial {
				// è¯ç»„åŒ¹é…æˆåŠŸ
				result = append(result, phrase)
				i += length
				matched = true
				break
			}
		}

		if matched {
			continue
		}

		// ğŸ¯ ç­–ç•¥2: å•å­—å¤„ç†
		char := runes[i]

		// æ£€æŸ¥æ˜¯å¦æ˜¯æ±‰å­—
		if isChineseChar(char) {
			// å•ä¸ªæ±‰å­—ä½œä¸ºä¸€ä¸ªè¯
			result = append(result, string(char))
			i++
		} else {
			// ğŸ¯ ç­–ç•¥3: éæ±‰å­—è¿ç»­åˆå¹¶
			start := i
			for i < len(runes) && !isChineseChar(runes[i]) {
				i++
			}
			result = append(result, string(runes[start:i]))
		}
	}

	return result
}

// SegmentForPinyin ä¸“é—¨ä¸ºæ‹¼éŸ³è½¬æ¢ä¼˜åŒ–çš„åˆ†è¯
// ğŸ”¥ é›†æˆäº†é«˜çº§ Tokenizer åŠŸèƒ½
// â­ é‡è¦ä¿®å¤ï¼šå…ˆåº”ç”¨ Tokenizerï¼Œå†è¿›è¡Œä¸­æ–‡åˆ†è¯
func (ls *LightweightSegmenter) SegmentForPinyin(text string) []string {
	if text == "" {
		return []string{}
	}

	// â­ ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ–‡æœ¬å…¨æ˜¯éæ±‰å­—ï¼ˆæ ‡ç‚¹ã€æ•°å­—ã€è‹±æ–‡ç­‰ï¼‰ï¼Œä¸æ‹†åˆ†
	// è¿™æ ·ä¿æŒä¸ npm pinyin v4 çš„ normal_pinyin è¡Œä¸ºä¸€è‡´
	// normal_pinyin ä¼šå°†è¿ç»­çš„éæ±‰å­—å­—ç¬¦ç´¯ç§¯ä¸ºä¸€ä¸ªå…ƒç´ 
	hasHanzi := false
	for _, r := range text {
		if isChineseChar(r) {
			hasHanzi = true
			break
		}
	}
	
	// å¦‚æœæ²¡æœ‰æ±‰å­—ï¼Œç›´æ¥è¿”å›æ•´ä¸ªæ–‡æœ¬ï¼ˆç´¯ç§¯éæ±‰å­—ï¼‰
	if !hasHanzi {
		return []string{text}
	}

	// â­ å…³é”®ä¿®å¤ï¼šå…ˆåº”ç”¨ Tokenizer ç®¡é“ï¼Œå†åˆ†è¯
	// è¿™æ ·å¯ä»¥ç¡®ä¿ URLã€æ•°å­—ã€è‹±æ–‡ç­‰å…ˆè¢«è¯†åˆ«ï¼Œä¸ä¼šè¢«åˆ†è¯æ‹†æ•£
	// 
	// æµç¨‹ï¼š
	// 1. æ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€ä¸ª Word è¾“å…¥
	// 2. Tokenizer ç®¡é“è¯†åˆ«å¹¶åˆ†ç¦» URL/æ•°å­—/è‹±æ–‡/æ ‡ç‚¹ç­‰
	// 3. å¯¹å‰©ä½™çš„ä¸­æ–‡éƒ¨åˆ†è¿›è¡Œåˆ†è¯
	
	// 1. åˆ›å»ºåˆå§‹ Wordï¼ˆæ•´ä¸ªæ–‡æœ¬ï¼‰
	initialWord := []tokenizer.Word{{
		W: text,
		P: 0,
		C: 0,
	}}

	// 2. åº”ç”¨ Tokenizer ç®¡é“
	// è¿™ä¼šè¯†åˆ« URLã€é‚®ç®±ã€æ•°å­—ã€è‹±æ–‡ã€æ ‡ç‚¹ç­‰
	tokenWords := ls.tokenizerPipeline.Process(initialWord)

	// 3. å¯¹æ¯ä¸ª tokenWord è¿›è¡Œè¿›ä¸€æ­¥å¤„ç†
	result := make([]string, 0, len(tokenWords))
	
	for _, tw := range tokenWords {
		// å¦‚æœå·²è¢«è¯†åˆ«ï¼ˆæœ‰è¯æ€§æ ‡æ³¨ï¼‰ï¼Œç›´æ¥ä¿ç•™
		if tw.P > 0 {
			result = append(result, tw.W)
		} else {
			// æœªè¯†åˆ«çš„éƒ¨åˆ†ï¼ˆå¯èƒ½æ˜¯ä¸­æ–‡ï¼‰ï¼Œè¿›è¡Œåˆ†è¯
			segments := ls.Segment(tw.W, "default")
			result = append(result, segments...)
		}
	}

	return result
}

// AddWord æ·»åŠ è‡ªå®šä¹‰è¯ï¼ˆè½»é‡çº§å®ç°ï¼šå¿½ç•¥ï¼Œå› ä¸ºæ²¡æœ‰ç‹¬ç«‹è¯å…¸ï¼‰
func (ls *LightweightSegmenter) AddWord(word string, freq int) {
	// è½»é‡çº§å®ç°ä¸ç»´æŠ¤ç‹¬ç«‹è¯å…¸ï¼Œç›´æ¥ä½¿ç”¨ PhrasesDict
	// å¦‚æœéœ€è¦åŠ¨æ€æ·»åŠ è¯ï¼Œå¯ä»¥è€ƒè™‘ç»´æŠ¤ä¸€ä¸ªå°å‹ map
}

// isChineseChar åˆ¤æ–­æ˜¯å¦æ˜¯ä¸­æ–‡å­—ç¬¦
func isChineseChar(r rune) bool {
	// CJK ç»Ÿä¸€æ±‰å­—
	if r >= 0x4E00 && r <= 0x9FFF {
		return true
	}
	// CJK æ‰©å±• A
	if r >= 0x3400 && r <= 0x4DBF {
		return true
	}
	// CJK æ‰©å±• B-F
	if r >= 0x20000 && r <= 0x2EBEF {
		return true
	}
	// CJK å…¼å®¹æ±‰å­—
	if r >= 0xF900 && r <= 0xFAFF {
		return true
	}
	return false
}

// ==========================================
// ğŸ”¥ å…¨å±€è½»é‡çº§åˆ†è¯å™¨æ¥å£ï¼ˆæ›¿ä»£ GSEï¼‰
// ==========================================

var (
	useLightweightSegmenter      = true // ğŸ”¥ æ§åˆ¶æ˜¯å¦ä½¿ç”¨è½»é‡çº§åˆ†è¯å™¨
	lightweightSegmenterInstance *LightweightSegmenter
)

// GetSegmenterLite è·å–è½»é‡çº§åˆ†è¯å™¨
// å†…å­˜å ç”¨å‡ ä¹ä¸º 0ï¼ˆåªå¤ç”¨å·²æœ‰çš„è¯ç»„å­—å…¸ï¼‰
func GetSegmenterLite() *LightweightSegmenter {
	if lightweightSegmenterInstance == nil {
		lightweightSegmenterInstance = NewLightweightSegmenter()
	}
	return lightweightSegmenterInstance
}

// SetUseLightweightSegmenter è®¾ç½®æ˜¯å¦ä½¿ç”¨è½»é‡çº§åˆ†è¯å™¨
// true: ä½¿ç”¨è½»é‡çº§åˆ†è¯å™¨ï¼ˆå†…å­˜å ç”¨ä½ï¼Œåˆ†è¯è´¨é‡ç•¥ä½ï¼‰
// false: ä½¿ç”¨ GSE åˆ†è¯å™¨ï¼ˆå†…å­˜å ç”¨é«˜ ~170MBï¼Œåˆ†è¯è´¨é‡é«˜ï¼‰
func SetUseLightweightSegmenter(useLite bool) {
	useLightweightSegmenter = useLite
}

// IsUsingLightweightSegmenter æ£€æŸ¥æ˜¯å¦æ­£åœ¨ä½¿ç”¨è½»é‡çº§åˆ†è¯å™¨
func IsUsingLightweightSegmenter() bool {
	return useLightweightSegmenter
}

// SegmenterInterface ç»Ÿä¸€çš„åˆ†è¯å™¨æ¥å£
type SegmenterInterface interface {
	Segment(text string, mode string) []string
	SegmentForPinyin(text string) []string
	AddWord(word string, freq int)
}

// GetUnifiedSegmenter è·å–ç»Ÿä¸€çš„åˆ†è¯å™¨ï¼ˆæ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©ï¼‰
// ğŸ”¥ è¿™æ˜¯æ¨èçš„è·å–åˆ†è¯å™¨çš„æ–¹å¼
func GetUnifiedSegmenter() SegmenterInterface {
	if useLightweightSegmenter {
		return GetSegmenterLite()
	}
	// å¦‚æœéœ€è¦ GSEï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
	// return GetGlobalSegmenter()

	// é»˜è®¤è¿”å›è½»é‡çº§åˆ†è¯å™¨
	return GetSegmenterLite()
}
